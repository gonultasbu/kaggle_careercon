{"cells":[{"metadata":{"_uuid":"03605938f3e3c6e55291b3bc348ed7d9ca574f64"},"cell_type":"markdown","source":"**Leaderboard Distribution**\n\nI have been doing this competetion for 4 days or so. \n\nThe main problem that everybody is (including me hitting my head through the WALL) facing is the **local cross validation is not matching the LeaderBoard.**\n\nIn this kernel I try to create a Validation Set which matches the leaderboard using LeaderBoard Probing done by **@donkeys and @ninoko.**\n\nThe leaderboard distributions are given in the discussion threads : \n\n1. https://www.kaggle.com/c/career-con-2019/discussion/84760\n2. https://www.kaggle.com/c/career-con-2019/discussion/85204\n\nA **big thank you** for wasting your 9 submissions for the greater GOOD. \n\nSo lets get into it."},{"metadata":{"_uuid":"d61baf7c351916d69b6924ec0268243b3c518fd3"},"cell_type":"markdown","source":"**Importing Tools**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom scipy import stats\nimport math\n\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport gc\nimport itertools","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"cd34d137fc03832c27aa84761a14a976b041fd7e"},"cell_type":"markdown","source":"**Reading Data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/X_train.csv\")\ntest = pd.read_csv(\"../input/X_test.csv\")\nlabel = pd.read_csv(\"../input/y_train.csv\")\nsub = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":54,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a45c58333cad54e5c1d0660e62a43a8adcbf955"},"cell_type":"code","source":"def reduce_mem_usage(df):\n    # iterate through all the columns of a dataframe and modify the data type\n    #   to reduce memory usage.        \n    \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","execution_count":55,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c90a25a9ff9dff71684c023a46b968ba9b32fe0b"},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":56,"outputs":[{"output_type":"stream","text":"Memory usage of dataframe is 48.37 MB\nMemory usage after optimization is: 14.88 MB\nDecreased by 69.2%\nMemory usage of dataframe is 48.45 MB\nMemory usage after optimization is: 14.91 MB\nDecreased by 69.2%\n","name":"stdout"}]},{"metadata":{"_uuid":"b894bb315c4a58ec68c64798de36ffc27e0207dc"},"cell_type":"markdown","source":"**Lets first check the Train Target Distribution** "},{"metadata":{"trusted":true,"_uuid":"357d5fe61dc93d48e567b238791f1873c702f1a3"},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nsns.countplot(label['surface'], order=label.surface.value_counts().index)\nplt.show()","execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"_uuid":"7b76703e85258516284b00b85560b853e27c4db5"},"cell_type":"markdown","source":"**We have very less records for \"hard_tiles\".**\n\nSo it would most probably be very difficult for our RandomForest, LightGBM to detect and correctly classify the hard_tiles."},{"metadata":{"_uuid":"21ae26a6d4c7ed2c7e00a40354b7284f22d594da"},"cell_type":"markdown","source":"The below is a distribution of the leaderboard probe given in the thread : https://www.kaggle.com/c/career-con-2019/discussion/84760"},{"metadata":{"_uuid":"3fb9723dd38eba1f6833df2b7740fa61e686fea7"},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/DoFc3mW.png\" />"},{"metadata":{"_uuid":"27b819cc37e9c494ca108ef76cdb8c0d38381746"},"cell_type":"markdown","source":"We can see that the leaderboard has quite different distibition than in our test.\n\nThe different distributions lie in : \n\n**1. High Local Predictions for Wood but less on the Leaderboard set.**\n\n**2. High Local Predictions for Tiled but less on the Leaderboard set.**\n\n**3. Low Local Predictions for Soft_Tiles but high on the Leaderboard set.**\n\n**4. Many records of type hard_tiles in the Leaderboard set. **\n\nLets instigate more. :/"},{"metadata":{"trusted":true,"_uuid":"1ace2a8295eb589ba7adbcfd563f384735f92d72"},"cell_type":"code","source":"# This is a dictionary consisting of the weights of the distributions for each target class from the below discussion thread.\n# https://www.kaggle.com/c/career-con-2019/discussion/85204#latest-496648\n\ndef create_valid_set(label):\n    # Lets try creating a validation set of 10% of the total size.\n    ldict = {\n        'concrete': 0.16,\n        'soft_pvc': 0.18,\n        'wood': 0.06,\n        'tiled': 0.03,\n        'fine_concrete': 0.10,\n        'hard_tiles_large_space': 0.12,\n        'soft_tiles': 0.23,\n        'carpet': 0.05,\n        'hard_tiles': 0.07,\n    }\n    score = 0\n    print(\"Required count of target classes for the Valid Set :: \")\n    for key, value in ldict.items():\n        score += value\n        print(key, int(value * 380)) # Multiplying by 380 i.e 10% of 3810 for our validation size of 10%.\n        ldict[key] = int(value * 380)\n    print(\"\\nTotal Weights of class :: \", score)\n    \n    # Grouping surface with group_id and the count attached to each surface.\n    ser = label.groupby(['surface'])['group_id'].value_counts()\n    ser = pd.DataFrame(ser)\n    ser.columns = ['count']\n    \n    # Maually creating the valid set using the counts using the required count and the count we have in the train set.\n    # This dictionary consists of the group_id for the required valid set. \n    cv_set = {\n        'concrete': [0],\n        'soft_pvc': [69],\n        'wood': [2],\n        'tiled': [28],\n        'fine_concrete': [36],\n        'hard_tiles_large_space': [16],\n        'soft_tiles': [4, 17],\n        'carpet': [52],\n        'hard_tiles': [27],\n    }\n\n    cv_size = 0\n    for key, value in cv_set.items():\n        print(key)\n        for i in value:\n            cv_size += label[label['group_id'] == i].shape[0]\n            print(\"\\nGot shape :: \", label[label['group_id'] == i].shape[0])\n        print(\"Expected shape :: \", ldict[key])\n    \n    val_df = pd.DataFrame()\n    for key, value in cv_set.items():\n        for i in value:\n            val_df = pd.concat([val_df, label[label['group_id'] == i]])\n    print(\"Valid Set Size :: \", val_df.shape[0])\n    \n    # We have only 1 group_id for the hard_tiles and it consists of only 21 records.\n    # So we have added the same group_id in the train as well as valid set. GROUP_ID = 27(for \"hard_tiles\")\n    hard_tiles_index = label[(label['surface'] == 'hard_tiles') & (label['group_id'] == 27)].index\n    \n    # Therefore train set = Total Set series_id - Valid Set series_id + Hard_Tiles.index\n    trn_series_id_list = list(set(label.series_id.unique()) - set(val_df.series_id.unique())) + hard_tiles_index.tolist()\n    \n    print(\"Train Set Distribution\")\n    print(label['surface'].iloc[trn_series_id_list].value_counts())\n    \n    print(\"Valid Set Distribution\")\n    print(label['surface'].iloc[val_df.index].value_counts())\n    \n    trn_df = label.iloc[trn_series_id_list]\n    \n    trn_df.set_index(['series_id'], inplace=True)\n    val_df.set_index(['series_id'], inplace=True)\n    \n    return trn_df, val_df","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb779b56342f6ea93198d739d22526a65a2cb9cf"},"cell_type":"code","source":"# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n    \n    return X, Y, Z","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"41b3e6c73866c83fb4cf8308b32a09c26f97b1f3"},"cell_type":"markdown","source":"**Highly Influenced by the kernels : **\n\n1. https://www.kaggle.com/prashantkikani/help-humanity-by-helping-robots/\n\n2. https://www.kaggle.com/jesucristo/my-best-helping-robots-0-72"},{"metadata":{"trusted":true,"_uuid":"7208ace91874fbdaa56c8d18f8d2b786d166b9f8"},"cell_type":"code","source":"def FE(data):\n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 +\n                             data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 +\n                             data['linear_acceleration_Z']**2)**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 +\n                            data['orientation_Z']**2)**0.5\n    data['z_planar_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2)**0.5\n    #Lets derive one more column since there is a relationship in velocity and acceleration\n    # v = u + a*t , u is initial velocty. if u = 0, then v = at means t = v/a\n    # but value of acceleration is more and value of velocity is less, lets do a/v relation\n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n    \n    # Deriving more feature, since we are reducing rows now, we should know min,max,mean values\n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])/2\n    return df","execution_count":60,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33bf57ffea4f94c55e4a1b6ed4a20edb16860f00","scrolled":true},"cell_type":"code","source":"%%time\n\ntrn_df, val_df = create_valid_set(label)\ntrain = FE(train)\ntest = FE(test)","execution_count":null,"outputs":[{"output_type":"stream","text":"Required count of target classes for the Valid Set :: \nconcrete 60\nsoft_pvc 68\nwood 22\ntiled 11\nfine_concrete 38\nhard_tiles_large_space 45\nsoft_tiles 87\ncarpet 19\nhard_tiles 26\n\nTotal Weights of class ::  1.0\nconcrete\n\nGot shape ::  57\nExpected shape ::  60\nsoft_pvc\n\nGot shape ::  70\nExpected shape ::  68\nwood\n\nGot shape ::  18\nExpected shape ::  22\ntiled\n\nGot shape ::  36\nExpected shape ::  11\nfine_concrete\n\nGot shape ::  36\nExpected shape ::  38\nhard_tiles_large_space\n\nGot shape ::  45\nExpected shape ::  45\nsoft_tiles\n\nGot shape ::  57\n\nGot shape ::  12\nExpected shape ::  87\ncarpet\n\nGot shape ::  11\nExpected shape ::  19\nhard_tiles\n\nGot shape ::  21\nExpected shape ::  26\nValid Set Size ::  363\nTrain Set Distribution\nconcrete                  722\nsoft_pvc                  662\nwood                      589\ntiled                     478\nfine_concrete             327\nhard_tiles_large_space    263\nsoft_tiles                228\ncarpet                    178\nhard_tiles                 21\nName: surface, dtype: int64\nValid Set Distribution\nsoft_pvc                  70\nsoft_tiles                69\nconcrete                  57\nhard_tiles_large_space    45\ntiled                     36\nfine_concrete             36\nhard_tiles                21\nwood                      18\ncarpet                    11\nName: surface, dtype: int64\n","name":"stdout"}]},{"metadata":{"_uuid":"344d96254bba501ed8dc2f6a1fb759ced1ae63db"},"cell_type":"markdown","source":"**Label Encoding our target classes**"},{"metadata":{"trusted":true,"_uuid":"fb4f15b66c7ede4b10705fe41cb686692dcc38ad"},"cell_type":"code","source":"le = LabelEncoder()\nlabel['surface'] = le.fit_transform(label['surface'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"499217024c918ae6eaa6ae4825b01aec69f9846c"},"cell_type":"markdown","source":"**Filling missing and infinite data by zeroes**"},{"metadata":{"trusted":true,"_uuid":"d253b11bb301c8a32d2282eeb0a141ecdb9ca43a"},"cell_type":"code","source":"train.fillna(0,inplace=True)\ntrain.replace(-np.inf,0,inplace=True)\ntrain.replace(np.inf,0,inplace=True)\ntest.fillna(0,inplace=True)\ntest.replace(-np.inf,0,inplace=True)\ntest.replace(np.inf,0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b825724ca37bfb1d95b2aa1b49471d5354c61ee7"},"cell_type":"code","source":"x_train = train.iloc[trn_df.index]\ny_train = label['surface'].iloc[trn_df.index]\n\nx_val = train.iloc[val_df.index]\ny_val = label['surface'].iloc[val_df.index]\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afd1a39b3e2926755f82928e87cc5e990091e58a"},"cell_type":"markdown","source":"**The function below check the Train, Test, and CV Scores of the trained model.**\n\n**Lets Check.**"},{"metadata":{"trusted":true,"_uuid":"a049655f72a99c5be96897d5ad626739681d51f9"},"cell_type":"code","source":"def lb_dist(model):\n    model.fit(x_train, y_train)\n    print(\"Train Acc :: \", accuracy_score(y_train, model.predict(x_train)))\n    print(\"Valid Acc :: \", accuracy_score(y_val, model.predict(x_val)))\n    print(\"CV Accuracy :: \", cross_val_score(rand, train, label['surface'], cv=5).mean())\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ef09df140c7a7d4253af8dae521fd3ff299fcf9"},"cell_type":"code","source":"rand = RandomForestClassifier(n_estimators=500, random_state=13)\nrand = lb_dist(rand)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"756522b8c1c07d3b1960943f1f7fd300205a8d7f"},"cell_type":"markdown","source":"Well those three lines state my pain of matching local CV to LeaderBoard.\n\nLets understand what those 3 lines say:\n1. Our Classifier is overfitting like HELL.\n2. Cross Validation is also misleading as : \n            a. Test Distribution is different from the Train Distribution.\n            b. In CV some splits might end up not even considering the \"hard_tiles\" class as it has only 21 records.\n            c. The \"hard_tiles\" and \"soft_tiles\" hold quite a lot in the leaderboard.\n\nSo we must find features which could help us classifying classes like  :  \"hard_tiles\", \"soft_tiles\", \"tiled\", \"wood\" as they have very different Train and LeaderBoard distributions."},{"metadata":{"_uuid":"d24db68c164b414f6a266c0605b2166f1a62cf6e"},"cell_type":"markdown","source":"**Lets check Confusion Matrix and see how our Classifier is doing.**"},{"metadata":{"trusted":true,"_uuid":"9424f772f6e17d5dbfadef1d55901eb941c614ae"},"cell_type":"code","source":"plot_confusion_matrix(y_val, rand.predict(x_val), classes=le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"015c6fb3cc2dda8bd6e0b66a4fa7dde6b7299990"},"cell_type":"markdown","source":"**Understanding the Confusion Matrix**\n\nThis plot states so much that is very valuable in what to do next to **increase our SCORE.**\n\nWhat I learnt from this plot : \n\n**1. The diagonal in confusion matrix should always look bright i.e more populated in the whole matrix which means that the True Labels and the Predicted Labels must be same (or we should maximise that).**\n\nExample of a **Good Confusion Matrix **\n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_002.png\" />\n\n**2. Worst Classified Classes are :**\n\n        a. Carpet - 1 correctly classified out of 11.\n        b. Concrete - 57 out of 57 (Might be Overfit).\n        c. Fine_Concrete - 0 out of 36 (WHAT!!!)\n        d. Hard_Tiles - 21 out of 21 (Might be Overfit).\n        e. Hard_Tiles_Large_Space - 0 out of 45 (WHAT!!!)\n        f. Soft_PVC - 66 out of 70 (GREAT) \n        g. Soft_Tiles - 53 ou tof 69 (I can live with that.)\n        h. Tiled - 33 out of 36 (GREAT)\n        i. Wood - 0 out of 18 (You kidding?)\n\n\n**This Confusion Matrix reveals the major faults in our RandomForestClassifier.**\n\n**So fixing these faults would/should mean an increase in score. (FOR SURE)**"},{"metadata":{"trusted":true,"_uuid":"9db770e28130ca7e6619e1b86017b0c7d689bdc3"},"cell_type":"code","source":"print(\"Accuracy Score :: \", accuracy_score(label['surface'], rand.predict(train)))\nplot_confusion_matrix(label['surface'], rand.predict(train), classes=le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4eccb76228734881304e7f4be07fb964259b3710"},"cell_type":"markdown","source":"**We can see that the Classifier is clearly overfitting by getting 96% in local CV and check the matrix!!**\n\n**That is one clean and bright diagonal.** (Just if I had that for the Leaderboard  **** *******__*******  )"},{"metadata":{"trusted":true,"_uuid":"658aae6e5cc9e3a15e9a3c736b5adc5b4ec237fa"},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=20)\npredicted = np.zeros((test.shape[0],9))\nmeasured= np.zeros((train.shape[0]))\nscore = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"625879332dd2e0eee83c213550473b33a8eea7d3"},"cell_type":"code","source":"for times, (trn_idx, val_idx) in enumerate(folds.split(train.values, label['surface'].values)):\n    model = RandomForestClassifier(n_estimators=500, random_state=13)\n    model.fit(train.iloc[trn_idx], label['surface'][trn_idx])\n    measured[val_idx] = model.predict(train.iloc[val_idx])\n    predicted += model.predict_proba(test)/folds.n_splits\n    score += model.score(train.iloc[val_idx], label['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times, model.score(train.iloc[val_idx], label['surface'][val_idx])))\n    gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b96c859884a46e5fa2a84200c01500505be6bd98"},"cell_type":"markdown","source":"**Submitting our Predictions**"},{"metadata":{"trusted":true,"_uuid":"3ee3fe46c226e02e084fba1584b7e5f5f2cba9e6"},"cell_type":"code","source":"sub['surface'] = le.inverse_transform(predicted.argmax(axis=1))\nsub.to_csv('rand_sub_10.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d00890b432bf289bf42f7f81097917c6cf6b4801"},"cell_type":"markdown","source":"**This submission scores 0.71 on LeaderBoard and we got a Validation Accuracy of 0.64. I think that might be good enough.**"},{"metadata":{"_uuid":"51fb7c8508922d8123fdcf653340c14d027bbb54"},"cell_type":"markdown","source":"**Checking our predictions for the Test Set**"},{"metadata":{"trusted":true,"_uuid":"40588c6294da12084ceb31bdcab5d9cc3286287c"},"cell_type":"code","source":"sub.surface.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"552a3c3e176142ffe1366645b79ea7b0b54123c5"},"cell_type":"markdown","source":"This was my first kernel, so please spare me and everybody is welcome for their suggestions in the comments.\n\n** **CONCLUSION** :: Understanding the confusion matrix and solving the problem of the imbalance in the target classes is defintly the way to go for increasing the Accuracy Score.**"},{"metadata":{"_uuid":"ddf640cc053f85507b207d60f80bd63cdec04bfe"},"cell_type":"markdown","source":"References :: \n\nFeature Engineering : \n\n1. https://www.kaggle.com/prashantkikani/help-humanity-by-helping-robots/\n\n2. https://www.kaggle.com/jesucristo/my-best-helping-robots-0-72\n\nLeaderBoard Probing\n\n1. https://www.kaggle.com/c/career-con-2019/discussion/84760\n\n2. https://www.kaggle.com/c/career-con-2019/discussion/85204\n \n\n"},{"metadata":{"_uuid":"67c418fc00b691f6d3beee357600d11ca4dba262"},"cell_type":"markdown","source":"**Thanks** for your time and **UPVOTE** if you liked it and decide to try the same. "},{"metadata":{"trusted":true,"_uuid":"1900fcdaa4c9319d5a52b1db04f82fcb27fff57a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}