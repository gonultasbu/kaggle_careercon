{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# TODO Test LSTM for Feature Extraction\n# Test both weighted and unweighted class approaches for better CV-LB estimation.\n# Test PCA After automated feature extractor.\n# Look at the feature importances of the autogenerated features.\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import rcParams\n%matplotlib inline\nle = preprocessing.LabelEncoder()\nimport re\nimport warnings\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport featuretools as ft \nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom tsfresh import extract_features\nfrom tsfresh import select_features\nfrom tsfresh.utilities.dataframe_functions import impute\nfrom sklearn.model_selection import StratifiedKFold, GroupShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nimport gc\nfrom tsfresh import extract_relevant_features\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_class_distribution(classes,tt, features,a=5,b=2):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9d2f8541d8a53344730cadb5a2b9752559d17fc","trusted":true},"cell_type":"code","source":"def balanced_cv(conf_mat):\n    assert len(conf_mat.shape) == 2\n    rows, columns = conf_mat.shape\n    intra_class_scores = np.zeros((1, columns))\n    column_sums = np.sum(conf_mat, axis=0)\n    for c in range(columns):\n        intra_class_scores[0][c] = float(conf_mat[c][c])/column_sums[c]\n        \n    balanced_score = np.mean(intra_class_scores)\n    return balanced_score\n'''\ndef lb_cv(conf_mat):\n    assert len(conf_mat.shape) == 2\n    rows, columns = conf_mat.shape\n    intra_class_scores = np.zeros((1, columns))\n    column_sums = np.sum(conf_mat, axis=0)\n    for c in range(columns):\n        intra_class_scores[0][c] = float(conf_mat[c][c])/column_sums[c]\n        \n    balanced_score = np.mean(intra_class_scores)\n    return balanced_score\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features,a=2,b=5):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(17,9))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_valid_set(label):\n    # Lets try creating a validation set of 10% of the total size.\n    ldict = {\n        'concrete': 0.16,\n        'soft_pvc': 0.18,\n        'wood': 0.06,\n        'tiled': 0.03,\n        'fine_concrete': 0.10,\n        'hard_tiles_large_space': 0.12,\n        'soft_tiles': 0.23,\n        'carpet': 0.05,\n        'hard_tiles': 0.07,\n    }\n    score = 0\n    print(\"Required count of target classes for the Valid Set :: \")\n    for key, value in ldict.items():\n        score += value\n        print(key, int(value * 380)) # Multiplying by 380 i.e 10% of 3810 for our validation size of 10%.\n        ldict[key] = int(value * 380)\n    print(\"\\nTotal Weights of class :: \", score)\n    \n    # Grouping surface with group_id and the count attached to each surface.\n    ser = label.groupby(['surface'])['group_id'].value_counts()\n    ser = pd.DataFrame(ser)\n    ser.columns = ['count']\n    \n    # Maually creating the valid set using the counts using the required count and the count we have in the train set.\n    # This dictionary consists of the group_id for the required valid set. \n    cv_set = {\n        'concrete': [0],\n        'soft_pvc': [69],\n        'wood': [2],\n        'tiled': [28],\n        'fine_concrete': [36],\n        'hard_tiles_large_space': [16],\n        'soft_tiles': [4, 17],\n        'carpet': [52],\n        'hard_tiles': [27],\n    }\n\n    cv_size = 0\n    for key, value in cv_set.items():\n        print(key)\n        for i in value:\n            cv_size += label[label['group_id'] == i].shape[0]\n            print(\"\\nGot shape :: \", label[label['group_id'] == i].shape[0])\n        print(\"Expected shape :: \", ldict[key])\n    \n    val_df = pd.DataFrame()\n    for key, value in cv_set.items():\n        for i in value:\n            val_df = pd.concat([val_df, label[label['group_id'] == i]])\n    print(\"Valid Set Size :: \", val_df.shape[0])\n    \n    # We have only 1 group_id for the hard_tiles and it consists of only 21 records.\n    # So we have added the same group_id in the train as well as valid set. GROUP_ID = 27(for \"hard_tiles\")\n    hard_tiles_index = label[(label['surface'] == 'hard_tiles') & (label['group_id'] == 27)].index\n    \n    # Therefore train set = Total Set series_id - Valid Set series_id + Hard_Tiles.index\n    trn_series_id_list = list(set(label.series_id.unique()) - set(val_df.series_id.unique())) + hard_tiles_index.tolist()\n    \n    print(\"Train Set Distribution\")\n    print(label['surface'].iloc[trn_series_id_list].value_counts())\n    \n    print(\"Valid Set Distribution\")\n    print(label['surface'].iloc[val_df.index].value_counts())\n    \n    trn_df = label.iloc[trn_series_id_list]\n    \n    trn_df.set_index(['series_id'], inplace=True)\n    val_df.set_index(['series_id'], inplace=True)\n    \n    return trn_df, val_df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/X_train.csv')\ntarget = pd.read_csv('../input/y_train.csv')\nsampl = pd.read_csv('../input/sample_submission.csv')\ntest = pd.read_csv('../input/X_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_df, val_df = create_valid_set(target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfc230e8283491c2cfe124b6c5c2a08f7560a14a","trusted":true},"cell_type":"code","source":"totalt = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([totalt, percent], axis=1, keys=['Total', 'Percent'])\n# missing_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4bdd12a7223d80e2d4c6c9be217935a5b80d526","trusted":true},"cell_type":"code","source":"totalt = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([totalt, percent], axis=1, keys=['Total', 'Percent'])\n# missing_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"380ea10f7bd221169de06c8652f3dbbc559c7ff4","trusted":true},"cell_type":"code","source":"# data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3d8dcd4ee59b754a2e606bd2d1442603f7102c3","trusted":true},"cell_type":"code","source":"# target.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6001b6cc5925e8223b2f1747f20863cdbd9a41d9","trusted":true},"cell_type":"code","source":"# test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"397df9d46c66cd5eab36b171661b603c3b4ab4fe","trusted":true},"cell_type":"code","source":"sns.set(style='darkgrid')\nsns.countplot(y = 'surface',\n              data = target,\n              order = target['surface'].value_counts().index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20d703b6ce001c09a776a31a3a95c01c65b3ed1d","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding\ntarget['surface'] = le.fit_transform(target['surface'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(target[(target['surface'] == 'hard_tiles') & (target['group_id'] == 27)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# print(data['row_id'].dtype)\ndata.drop(['row_id'],axis=1)\ndata = data.astype('float64')\ntest.drop(['row_id'],axis=1)\ntest = test.astype('float64')\n'''\ndata[data['series_id'] == 0].plot(subplots=True, sharex=True, figsize=(10,10))\nplt.show()\n'''\ndata = extract_relevant_features(data, target['surface'], column_id='series_id', column_sort='measurement_number')\n# data = extract_features(data, column_id=\"series_id\", column_sort=\"measurement_number\")\ntest = extract_features(test, column_id=\"series_id\", column_sort=\"measurement_number\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impute(data)\ntest = test[data.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.iloc[trn_df.index]\ny_train = label['surface'].iloc[trn_df.index]\n\nx_val = train.iloc[val_df.index]\ny_val = label['surface'].iloc[val_df.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lb_dist(model):\n    model.fit(x_train, y_train)\n    print(\"Train Acc :: \", accuracy_score(y_train, model.predict(x_train)))\n    print(\"Valid Acc :: \", accuracy_score(y_val, model.predict(x_val)))\n    print(\"CV Accuracy :: \", cross_val_score(rand, train, label['surface'], cv=5).mean())\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"impute(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand = RandomForestClassifier(n_estimators=200, random_state=1337)\nrand = lb_dist(rand)\nprint(\"Accuracy Score :: \", accuracy_score(label['surface'], rand.predict(train)))\nplot_confusion_matrix(label['surface'], rand.predict(train), classes=le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffc836028e31ba704b301305afc08fc4166e8dee","trusted":true},"cell_type":"code","source":"split_count = 5\nfolds = StratifiedKFold(n_splits=5, shuffle=True)\n# folds = GroupShuffleSplit(n_splits=split_count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bde524f57f7e5b8c6250c3178147445adb6682e","trusted":true},"cell_type":"code","source":"predicted = np.zeros((test.shape[0],9))\n# measured= np.zeros((data.shape[0]))\nscore = 0\nbalanced_cv_sum = 0\n# Balanced model training gives slightly worse results on pubLB.\nmodel = RandomForestClassifier(n_estimators=200, class_weight = 'balanced')\n# model = XGBClassifier(tree_method='gpu_hist')\n# model = LGBMClassifier(device='gpu')\nfor times, (trn_idx, val_idx) in enumerate(folds.split(data.values,target['surface'].values)):\n    model.fit(data.iloc[trn_idx],target['surface'][trn_idx])\n    # measured[val_idx] = model.predict(data.iloc[val_idx])\n    # predicted += model.predict_proba(test)/folds.n_splits\n    score += model.score(data.iloc[val_idx],target['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times,model.score(data.iloc[val_idx],target['surface'][val_idx])))\n    cm = confusion_matrix(model.predict(data.iloc[val_idx]), target['surface'].iloc[val_idx])\n    # plt.figure()\n    # sns.heatmap(cm, annot=True, cmap=\"YlOrBr\")\n    '''\n    importances = model.feature_importances_\n    indices = np.argsort(importances)\n    features = data.columns\n\n    hm = 30\n    plt.figure(figsize=(7, 10))\n    plt.title('Feature Importances')\n    plt.barh(range(len(indices[:hm])), importances[indices][:hm], color='b', align='center')\n    plt.yticks(range(len(indices[:hm])), [features[i] for i in indices])\n    plt.xlabel('Relative Importance')\n    plt.show()\n    '''\n    balscore = balanced_cv(cm)\n    print('balanced cv score', balscore)\n    balanced_cv_sum +=balscore\n    gc.collect()\nprint('Avg Accuracy', score / folds.n_splits)\nprint('Avg Balanced Accuracy', balanced_cv_sum / folds.n_splits)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7b987ad134006f8e162eacac0ca4aaab64c88df","trusted":true},"cell_type":"code","source":"model.fit(data, target['surface'])\npredicted = model.predict_proba(test)\nsampl['surface'] = le.inverse_transform(predicted.argmax(axis=1))\nsampl.to_csv('submission.csv', index=False)\n# sampl.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}