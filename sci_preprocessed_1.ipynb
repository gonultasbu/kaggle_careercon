{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy import stats\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "import itertools\n",
    "from tsfresh.feature_extraction.feature_calculators import kurtosis, skewness, mean_abs_change, mean_change, sample_entropy, abs_energy, absolute_sum_of_changes, quantile\n",
    "from tsfresh.feature_selection.selection import select_features\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"X_train.csv\")\n",
    "test = pd.read_csv(\"X_test.csv\")\n",
    "label = pd.read_csv(\"y_train.csv\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    # iterate through all the columns of a dataframe and modify the data type\n",
    "    #   to reduce memory usage.        \n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n",
    "    cm = confusion_matrix(truth, pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix', size=15)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid_set(label):\n",
    "    # Lets try creating a validation set of 10% of the total size.\n",
    "    ldict = {\n",
    "        'concrete': 0.16,\n",
    "        'soft_pvc': 0.18,\n",
    "        'wood': 0.06,\n",
    "        'tiled': 0.03,\n",
    "        'fine_concrete': 0.10,\n",
    "        'hard_tiles_large_space': 0.12,\n",
    "        'soft_tiles': 0.23,\n",
    "        'carpet': 0.05,\n",
    "        'hard_tiles': 0.07,\n",
    "    }\n",
    "    score = 0\n",
    "    print(\"Required count of target classes for the Valid Set :: \")\n",
    "    for key, value in ldict.items():\n",
    "        score += value\n",
    "        print(key, int(value * 380)) # Multiplying by 380 i.e 10% of 3810 for our validation size of 10%.\n",
    "        ldict[key] = int(value * 380)\n",
    "    print(\"\\nTotal Weights of class :: \", score)\n",
    "    \n",
    "    # Grouping surface with group_id and the count attached to each surface.\n",
    "    ser = label.groupby(['surface'])['group_id'].value_counts()\n",
    "    ser = pd.DataFrame(ser)\n",
    "    ser.columns = ['count']\n",
    "    \n",
    "    # Maually creating the valid set using the counts using the required count and the count we have in the train set.\n",
    "    # This dictionary consists of the group_id for the required valid set. \n",
    "    cv_set = {\n",
    "        'concrete': [0],\n",
    "        'soft_pvc': [69],\n",
    "        'wood': [2],\n",
    "        'tiled': [28],\n",
    "        'fine_concrete': [36],\n",
    "        'hard_tiles_large_space': [16],\n",
    "        'soft_tiles': [4, 17],\n",
    "        'carpet': [52],\n",
    "        'hard_tiles': [27],\n",
    "    }\n",
    "\n",
    "    cv_size = 0\n",
    "    for key, value in cv_set.items():\n",
    "        print(key)\n",
    "        for i in value:\n",
    "            cv_size += label[label['group_id'] == i].shape[0]\n",
    "            print(\"\\nGot shape :: \", label[label['group_id'] == i].shape[0])\n",
    "        print(\"Expected shape :: \", ldict[key])\n",
    "    \n",
    "    val_df = pd.DataFrame()\n",
    "    for key, value in cv_set.items():\n",
    "        for i in value:\n",
    "            val_df = pd.concat([val_df, label[label['group_id'] == i]])\n",
    "    print(\"Valid Set Size :: \", val_df.shape[0])\n",
    "    \n",
    "    # We have only 1 group_id for the hard_tiles and it consists of only 21 records.\n",
    "    # So we have added the same group_id in the train as well as valid set. GROUP_ID = 27(for \"hard_tiles\")\n",
    "    hard_tiles_index = label[(label['surface'] == 'hard_tiles') & (label['group_id'] == 27)].index\n",
    "    \n",
    "    # Therefore train set = Total Set series_id - Valid Set series_id + Hard_Tiles.index\n",
    "    trn_series_id_list = list(set(label.series_id.unique()) - set(val_df.series_id.unique())) + hard_tiles_index.tolist()\n",
    "    \n",
    "    print(\"Train Set Distribution\")\n",
    "    print(label['surface'].iloc[trn_series_id_list].value_counts())\n",
    "    \n",
    "    print(\"Valid Set Distribution\")\n",
    "    print(label['surface'].iloc[val_df.index].value_counts())\n",
    "    \n",
    "    trn_df = label.iloc[trn_series_id_list]\n",
    "    \n",
    "    trn_df.set_index(['series_id'], inplace=True)\n",
    "    val_df.set_index(['series_id'], inplace=True)\n",
    "    \n",
    "    return trn_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE(data):\n",
    "    df = pd.DataFrame()\n",
    "    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 +\n",
    "                             data['angular_velocity_Z']**2)** 0.5\n",
    "    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 +\n",
    "                             data['linear_acceleration_Z']**2)**0.5\n",
    "    # data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 +\n",
    "    #                     data['orientation_Z']**2)**0.5\n",
    "    # data['z_planar_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2)**0.5\n",
    "    # data['z_planar_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2)**0.5\n",
    "    #Lets derive one more column since there is a relationship in velocity and acceleration\n",
    "    # v = u + a*t , u is initial velocty. if u = 0, then v = at means t = v/a\n",
    "    # but value of acceleration is more and value of velocity is less, lets do a/v relation\n",
    "    # data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n",
    "    \n",
    "    # Deriving more feature, since we are reducing rows now, we should know min,max,mean values\n",
    "    for col in data.columns:\n",
    "        if col in ['row_id','series_id','measurement_number']:\n",
    "            continue\n",
    "        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n",
    "        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n",
    "        # df[col + '_max'] = data.groupby(['series_id'])[col].max()\n",
    "        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n",
    "        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n",
    "        df[col + '_maxtoMin'] = data.groupby(['series_id'])[col].max() / data.groupby(['series_id'])[col].min()\n",
    "        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: mean_abs_change(x))\n",
    "        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n",
    "        # df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n",
    "        df[col + '_kurtosis'] = data.groupby(['series_id'])[col].apply(lambda x: kurtosis(x))\n",
    "        df[col + '_skewness'] = data.groupby(['series_id'])[col].apply(lambda x: skewness(x))\n",
    "        # df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])/2\n",
    "        # df[\"diff_\"+col] = df[col]-df[col].shift(1)\n",
    "        # df[\"ma_\"+col] = np.square(abs(df[col]-df[col].rolling(8).mean()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 48.37 MB\n",
      "Memory usage after optimization is: 14.88 MB\n",
      "Decreased by 69.2%\n",
      "Memory usage of dataframe is 48.45 MB\n",
      "Memory usage after optimization is: 14.91 MB\n",
      "Decreased by 69.2%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmg_df = pd.read_csv('pmg_df.csv')\n",
    "pmg_ts_df = pd.read_csv('pmg_ts_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required count of target classes for the Valid Set :: \n",
      "concrete 60\n",
      "soft_pvc 68\n",
      "wood 22\n",
      "tiled 11\n",
      "fine_concrete 38\n",
      "hard_tiles_large_space 45\n",
      "soft_tiles 87\n",
      "carpet 19\n",
      "hard_tiles 26\n",
      "\n",
      "Total Weights of class ::  1.0\n",
      "concrete\n",
      "\n",
      "Got shape ::  57\n",
      "Expected shape ::  60\n",
      "soft_pvc\n",
      "\n",
      "Got shape ::  70\n",
      "Expected shape ::  68\n",
      "wood\n",
      "\n",
      "Got shape ::  18\n",
      "Expected shape ::  22\n",
      "tiled\n",
      "\n",
      "Got shape ::  36\n",
      "Expected shape ::  11\n",
      "fine_concrete\n",
      "\n",
      "Got shape ::  36\n",
      "Expected shape ::  38\n",
      "hard_tiles_large_space\n",
      "\n",
      "Got shape ::  45\n",
      "Expected shape ::  45\n",
      "soft_tiles\n",
      "\n",
      "Got shape ::  57\n",
      "\n",
      "Got shape ::  12\n",
      "Expected shape ::  87\n",
      "carpet\n",
      "\n",
      "Got shape ::  11\n",
      "Expected shape ::  19\n",
      "hard_tiles\n",
      "\n",
      "Got shape ::  21\n",
      "Expected shape ::  26\n",
      "Valid Set Size ::  363\n",
      "Train Set Distribution\n",
      "concrete                  722\n",
      "soft_pvc                  662\n",
      "wood                      589\n",
      "tiled                     478\n",
      "fine_concrete             327\n",
      "hard_tiles_large_space    263\n",
      "soft_tiles                228\n",
      "carpet                    178\n",
      "hard_tiles                 21\n",
      "Name: surface, dtype: int64\n",
      "Valid Set Distribution\n",
      "soft_pvc                  70\n",
      "soft_tiles                69\n",
      "concrete                  57\n",
      "hard_tiles_large_space    45\n",
      "fine_concrete             36\n",
      "tiled                     36\n",
      "hard_tiles                21\n",
      "wood                      18\n",
      "carpet                    11\n",
      "Name: surface, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "trn_df, val_df = create_valid_set(label)\n",
    "train = FE(train)\n",
    "test = FE(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "label['surface'] = le.fit_transform(label['surface'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(0,inplace=True)\n",
    "train.replace(-np.inf,0,inplace=True)\n",
    "train.replace(np.inf,0,inplace=True)\n",
    "test.fillna(0,inplace=True)\n",
    "test.replace(-np.inf,0,inplace=True)\n",
    "test.replace(np.inf,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = pd.DataFrame(pmg_df['group_id'])\n",
    "x_train = train.iloc[trn_df.index]\n",
    "y_train = label['surface'].iloc[trn_df.index]\n",
    "\n",
    "x_val = train.iloc[val_df.index]\n",
    "y_val = label['surface'].iloc[val_df.index]\n",
    "\n",
    "# print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_groups['group_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = RandomForestClassifier(n_estimators=200, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3810, 9)\n",
      "(3810, 1)\n"
     ]
    }
   ],
   "source": [
    "rand.fit(x_train, y_train)\n",
    "train_probs = rand.predict_proba(train)\n",
    "print(train_probs.shape)\n",
    "print(train_groups.shape)\n",
    "preds_groups_df = pd.DataFrame(train_probs).join(train_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a8e675b82008>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mgroup_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_groups_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreds_groups_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'group_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mg_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpredicted_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_groups_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreds_groups_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'group_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mg_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mfull_train_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup_indices\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Grouped CV Acc:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_train_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'surface'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "full_train_predictions = np.zeros((train_probs.shape[0]), dtype='int32')\n",
    "for g_id in range(preds_groups_df['group_id'].nunique()):\n",
    "    group_indices = preds_groups_df[preds_groups_df['group_id']==g_id].index\n",
    "    predicted_value = preds_groups_df[preds_groups_df['group_id']==g_id].mean()[[0,1,2,3,4,5,6,7,8]].idxmax()\n",
    "    full_train_predictions[group_indices] = int(predicted_value)\n",
    "\n",
    "print(\"Grouped CV Acc:\", accuracy_score(le.inverse_transform(full_train_predictions[val_df.index]),le.inverse_transform(label['surface'].iloc[val_df.index])))\n",
    "print(\"Standard CV Acc:\", accuracy_score(le.inverse_transform(label['surface'].iloc[val_df.index]), le.inverse_transform(rand.predict(train.iloc[val_df.index]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_id = 22\n",
    "full_train_predictions = np.zeros((train_probs.shape[0]))\n",
    "# print(\"Indices\", preds_groups_df[preds_groups_df['group_id']==g_id].index)\n",
    "# print(\"Prediction\",preds_groups_df[preds_groups_df['group_id']==g_id].mean()[[1,2,3,4,5,6,7,8]].idxmax())\n",
    "# full_train_predictions[preds_groups_df[preds_groups_df['group_id']==g_id].index] = preds_groups_df[preds_groups_df['group_id']==g_id].mean()[[1,2,3,4,5,6,7,8]].idxmax()\n",
    "print(preds_groups_df[preds_groups_df['group_id']==g_id].mean()[[0,1,2,3,4,5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Implement a surface based success metric rather than group based.\n",
    "# If the grouping is successful, then the feature engineering part causes the error. Try signal processing.\n",
    "# Some groups need to be further split, looking at the probabilities. Use this information.\n",
    "# Implement the same for test set.\n",
    "# Analyze which classes are getting misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = train_groups[train_groups['group_id']==741].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_grouped_count = 0\n",
    "total_count = 0\n",
    "singular_groups = 0\n",
    "for c in range(train_groups['group_id'].nunique()):\n",
    "    indices = train_groups[train_groups['group_id']==c].index\n",
    "    correctly_grouped_count += label['group_id'].iloc[indices].value_counts().iloc[0]\n",
    "    total_count += label['group_id'].iloc[indices].value_counts().sum()\n",
    "    if train_groups[train_groups['group_id']==c].index.size == 1 or 0: singular_groups+=1\n",
    "\n",
    "print(correctly_grouped_count/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "correctly_grouped_count = 0\n",
    "total_count = 0\n",
    "singular_groups = 0\n",
    "for c in range(train_groups['group_id'].nunique()):\n",
    "    indices = train_groups[train_groups['group_id']==c].index\n",
    "    if train_groups[train_groups['group_id']==c].index.size == 1 or 0: singular_groups+=1\n",
    "\n",
    "print(singular_groups)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmg_df = pd.read_csv(\"pmg_df.csv\")\n",
    "x_train = pd.read_csv(\"x_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series_id          0.000000\n",
      "group_id           0.000000\n",
      "distance           0.000488\n",
      "nearest_point    475.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "x_train_g = x_train.groupby(['series_id'])\n",
    "y_train_g = y_train.groupby(['series_id'])\n",
    "print(pmg_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id                      996_127\n",
      "measurement_number              127\n",
      "orientation_X              -0.75996\n",
      "orientation_Y               -0.6326\n",
      "orientation_Z              -0.10481\n",
      "orientation_W              -0.10628\n",
      "angular_velocity_X        0.0051752\n",
      "angular_velocity_Y       -0.0088096\n",
      "angular_velocity_Z       0.00087172\n",
      "linear_acceleration_X      -0.44258\n",
      "linear_acceleration_Y        4.1739\n",
      "linear_acceleration_Z       -9.6836\n",
      "Name: 996, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = 996\n",
    "print(x_train.groupby(['series_id']).last().iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id                     1116_0\n",
      "measurement_number              0\n",
      "orientation_X            -0.92772\n",
      "orientation_Y             0.34254\n",
      "orientation_Z            0.045284\n",
      "orientation_W            -0.14123\n",
      "angular_velocity_X      -0.058909\n",
      "angular_velocity_Y       0.078028\n",
      "angular_velocity_Z       -0.11527\n",
      "linear_acceleration_X     -2.4032\n",
      "linear_acceleration_Y      5.5525\n",
      "linear_acceleration_Z     -6.6973\n",
      "Name: 1116, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = 1116\n",
    "print(x_train.groupby(['series_id']).first().iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "fine_concrete\n"
     ]
    }
   ],
   "source": [
    "a=1166\n",
    "print(pmg_df['nearest_point'].iloc[a])\n",
    "print(y_train['surface'].iloc[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix mislabeling due to group number increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
